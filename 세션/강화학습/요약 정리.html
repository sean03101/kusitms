<html>
<head>
  <title>Evernote Export</title>
  <basefont face="Segoe UI" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/309091 (ko-KR, DDL); Windows/10.0.0 (Win64);"/>
  <style>
    body, td {
      font-family: Segoe UI;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="446"/>

<div>
<span><div><div style="text-align: justify;"><span style="font-size: 13pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">1. Reinforcement learning 개요</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">머신러닝 방법 중 하나, 데이터가 없어도 학습 가능</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"> </span><span style="font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"><img src="요약 정리_files/Image.png" type="image/png" data-filename="Image.png"/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">주요 이슈</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"> 1) credit assignment 문제</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> : agent가 점수를 받기 직전에 취한 액션들은 보상과 직접적인 관련이 없어</span> <span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">보상 발생 시점   이전의 일련의 액션들 중 보상에 기여한 액션들이 무엇인지 고려해야 함</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             -&gt;</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Bellman equation</span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">으로 해결 가능</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">2) exploration-exploitation 문제</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"> </span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">: agent가 행동을 취하고 보상을 받았을 때, 그 보상만 추구해야 하는지 새로운 탐색을 해볼   지 고민해야 함</span></div><div style="text-align: justify;"><span style="font-size: 12pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">           </span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">-&gt;</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">random noise, e-greedy</span> <span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">방법 등으로 해결 가능</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 18pt; color: rgb(227, 0, 0); font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">미완성</span></div><div style="text-align: justify;"></div><div style="text-align: justify;"><span style="font-size: 14pt; font-weight: bold;">가치 평가 방법 3가지</span><span style="font-size: 14pt;"> </span></div><div style="text-align: justify;"><br/></div><div style="text-align: justify;"></div><div style="text-align: justify;"><span style="font-weight: bold;">    DP (Dynamic Programming)</span> : 하나의 문제를 더 작은 문제들로 나눠서 반복해서 푸는 방법</div><ul><li><div style="text-align: justify;">정교한 모델 필요</div></li><li><div style="text-align: justify;">실시간 처리 가능</div></li><li><div style="text-align: justify;">계산 복잡도가 큼</div></li></ul><div style="text-align: justify;"><br/></div><div style="text-align: justify;"><span style="font-weight: bold;">    MC (Monte Carlo)</span> :  모델 없이 일단 행동을 해보고 매 에피소드마다 얻은 reward를 평균내서 value function를 만들어가는 방법</div><ul><li><div style="text-align: justify;">모델 필요 없음</div></li><li><div style="text-align: justify;">실시간 처리 불가</div></li></ul><div style="text-align: justify;">    - 에피소드 내에서 reward/방문 카운트를 계속 세서 평균 -&gt; 대수의 법칙에 의해 가치 함수에 근접함</div><div style="text-align: justify;"><br/></div><div style="text-align: justify;">        <img src="요약 정리_files/Image [1].png" type="image/png" data-filename="Image.png"/></div><div style="text-align: justify;"><br/></div><div style="text-align: justify;"><span style="font-weight: bold;">    TD (Temporal Difference)</span> : MC에서 실시간 처리 반영 ( 한 스텝 후 reward를 예측해서 현재 reward를 뺌??)</div><ul><li><div style="text-align: justify;">모델 필요 없음</div></li></ul><div style="text-align: justify;"><br/></div><div style="text-align: justify;">        <img src="요약 정리_files/Image [2].png" type="image/png" data-filename="Image.png"/></div><div style="text-align: justify;"><br/></div><div style="text-align: justify;"><span style="font-weight: bold;">   </span><span style="font-weight: bold;">MC 특징</span> : 매 에피소드마다 처리, 낮은 편차, 높은 분산 (low bias, high variance) </div><div style="text-align: justify;"><span style="font-weight: bold;">   TD 특징</span> : 매 스텝마다 처리, 높은 편차, 낮은 분산 (high bias, low variance)  </div><div style="text-align: justify;"><span style="font-weight: bold;">    </span></div><div style="text-align: justify;"><span style="font-size: 12pt;"><img src="요약 정리_files/Image [3].png" type="image/png" data-filename="Image.png"/></span></div><div style="text-align: justify;"><span style="font-size: 12pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 12pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 12pt; font-family: 함초롬바탕; font-weight: bold;">Q-function</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     - 상태 s에서 액션 a를 수행한 후, 게임이 종료될 때 가질 수 있는 최고 점수</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     - Q-learning에서는 행렬, Q-network에서는 신경망으로 구현됨</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">                 (Q function 식 :</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [4].png" type="image/png" data-filename="Image.png" width="144"/> </span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">)</span></div><div style="text-align: justify;"><span style="font-size: 12pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">    =&gt; </span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">Q-function을 따르면 최적의 policy 시행 가능 =&gt;</span> <img src="요약 정리_files/Image [5].png" type="image/png" data-filename="Image.png" style="line-height:160%;" width="153"/></div><div style="text-align: justify;"><br/></div><div style="text-align: justify;"><span style="font-size: 13pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">2</span><span style="font-size: 13pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; font-weight: bold;">. Q-learning</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     -</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Bellman equation</span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">을 반복적으로 사용하여 Q-function을 근사하는 알고리즘</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">    (벨만 최적 방정식 :</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [6].png" type="image/png" data-filename="Image.png" width="166"/></span> <span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">=</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [7].png" type="image/png" data-filename="Image.png" width="172"/></span></div><div style="text-align: justify;"><br/></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">    따라서</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Q-function</span> <span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">:</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [8].png" type="image/png" data-filename="Image.png" width="93"/> <img src="요약 정리_files/Image [9].png" type="image/png" data-filename="Image.png" width="8"/> <img src="요약 정리_files/Image [10].png" type="image/png" data-filename="Image.png" width="98"/></span></div><div style="text-align: justify;"></div><div style="text-align: justify;"><span style="font-size: 12pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">           -&gt; non deterministic 환경에서 학습 불가</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">             deterministic 환경에서도 가능한 식 :</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">                           </span><span style="line-height: 160%;"><img src="요약 정리_files/Image [11].png" type="image/png" data-filename="Image.png" width="334"/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">                           </span><span style="line-height: 160%;"><img src="요약 정리_files/Image [12].png" type="image/png" data-filename="Image.png" width="296"/></span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">     </span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">    단점</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     - 비교적 작은 환경에만 사용 가능</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 13pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">3. Q-network</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> - Q-learning에서 Q-function을 근사할 때 Q-learning의 행렬 형태를 사용하지 않고 신경망을 사용하는 방법</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     </span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">장점</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">      - 큰 환경에서도 구현 가능하게 됨</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">      하지만 아래의</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">문제점</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">2가지</span><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">로 단순히 Q-network만으로는 성능이 미약하다</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             1)</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Correlation between samples</span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             - 강화학습에서의 학습 데이터는 시간의 흐름에 따라 순차적으로 수집되고, 이 순차적인 데이터는 높은 correlation을 갖게 된다.</span></div><div style="text-align: justify;"><span style="font-size: 10pt;"><br/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             </span><span style="font-family: 함초롬바탕; line-height: 160%;"><img src="요약 정리_files/Image [13].png" type="image/png" data-filename="Image.png"/></span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             2)</span> <span style="font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Non-stationary targets</span></div><div style="text-align: justify;"><span style="font-size: 12pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">            </span><span style="line-height: 160%;"><img src="요약 정리_files/Image [14].png" type="image/png" data-filename="Image.png" width="368"/></span></div><div style="text-align: justify;"><br/></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             위 식(Q-network에서 MSE를 사용한 loss function)을 봤을 때 y-target으로 오차를 구하고 있다. </span></div><div style="text-align: justify;"><span style="font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">             y 내부에 Q 식이 있어서 Q를 업데이트하면 y또한 업데이트 되버린다. (불안정성)</span></div><div style="text-align: justify;"><span style="font-size: 10pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; line-height: 160%;">             </span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 12pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">4. DQN(Deep Q Network)</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> (DeepMind의 &quot;Playing Atari with Deep Reinforcement Learning&quot; 논문에서 제안)</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> - 위 Q-network 문제를 해결하기 위해 Q-network에 다음의 2가지 방법을 추가하였다.</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     1.</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Experience replay (Replay memory)</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">     </span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">- 별도의 데이터 공간을 만들어 mini batch을 구성하여 학습을 진행한다.</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     2.</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Fixed Q-targets</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">     </span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">-  Q(s,a;θ)와 같은 네트워크 구조이지만 다른 파라미터를 가진(독립적인) target network Q^(s,a;θ−)를 만들고 이를 y에 이용한다.</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">이 외에도 성능 향상을 위한 두 가지 방법을 추가하였다.</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     1.</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">CNN</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">      -</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">내부 조건을 학습하지 않고, 이미지 픽셀을 CNN에 거쳐 학습한다.</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">     2.</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Gradient clipping</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">      - Network의 loss function에 대한 gradient가 -1 보다 작을때는 -1로, 1보다 클 때는 1로 조정해준다. =&gt; 학습 과정 안정화</span></div><div style="margin: 0mm 0mm 0mm 5mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><hr/><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>출처</span> <span style="font-size: 10pt; text-indent: -5mm; color: rgb(128, 0, 128); font-family: 함초롬바탕; line-height: 160%; text-decoration: underline;">https://www.youtube.com/watch?v=-MQHlnk5GOU</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-size: 10pt; text-indent: -5mm; color: rgb(0, 0, 255); font-family: 함초롬바탕; line-height: 160%; text-decoration: underline;"><span>    </span><span>    </span><span>    </span>https://blog.naver.com/jk96491/22185146402</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 8pt; font-family: Arial; line-height: 160%;"><span>    </span>[토크ON세미나] 강화학습 2 (policy gradient) 3강 - Policy Gradient II - REINFORCE 실습 | T아카데미</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"><span>    </span>Value–Based(Q-learning , DQN)의 단점</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>- 수렴이 늦다</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>- 불안정하다(Q-function이 조금만 달라져도 행동이 변함)</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>- 많은 행동(action)을 가진 환경이나 연속적(continuous) 환경에서 성능이 낮다</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>- 확률(Stochastic) 개념 도입이 불가능하다(반드시 하나의 optimal한 action으로 수렴한다( ex : 가위바위보)</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 14pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; line-height: 160%;"><span>    </span>5.</span> <span style="text-indent: -5mm; font-size: 14pt; color: rgb(0, 0, 255); font-family: 함초롬바탕; line-height: 160%;">Policy Gradient</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>매 action에 확률을 할당해 확률에 따라 행동한다.</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>여기서의 확률은 보상을 최대화할 수 있는 확률이다</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"><span>    </span>Reinforce algorithm</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> <span>    </span></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">목표</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">:</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">목적 함수</span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">(Reward를 받을 확률)를</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">Gradient Accent</span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">를 이용해</span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;">최대화</span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">한다.</span></div><div style="margin-top: 0mm; margin-right: 0mm; margin-bottom: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-weight: bold;"><span>    </span>목적 함수</span> : <span style="line-height: 160%;"><img src="요약 정리_files/Image [15].png" type="image/png" data-filename="Image.png" width="165"/></span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">,</span> <span style="text-indent: -5mm; font-size: 7pt; font-family: 함초롬바탕; line-height: 160%;">( U = object function, R= Return )</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; font-weight: bold; line-height: 160%;"><span>    </span>Gradient Accent</span><span style="text-indent: -5mm; font-size: 6pt; font-family: 함초롬바탕; line-height: 160%;"> </span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">:</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [16].png" type="image/png" data-filename="Image.png" width="132"/></span> <span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">,</span> <span style="text-indent: -5mm; font-size: 7pt; font-family: 함초롬바탕; line-height: 160%;">(</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [17].png" type="image/png" data-filename="Image.png" width="10"/></span> <span style="text-indent: -5mm; font-size: 7pt; font-family: 함초롬바탕; line-height: 160%;">= learning rate )</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 7pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> <span>    </span>Gradient Accent를 하기 위해서는</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [18].png" type="image/png" data-filename="Image.png" width="16"/></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">U(</span><span style="line-height: 160%;"><img src="요약 정리_files/Image [19].png" type="image/png" data-filename="Image.png" width="8"/></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">)를 알아야 가능하므로</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"> <span>    </span>위 U(</span><span style="line-height: 160%;"><img src="요약 정리_files/Image [20].png" type="image/png" data-filename="Image.png" width="8"/></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">)의 식을 수학식으로 전개하면(생략 ..) 결론적으로 다음과 같은 형태의</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [21].png" type="image/png" data-filename="Image.png" width="16"/></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">U(</span><span style="line-height: 160%;"><img src="요약 정리_files/Image [22].png" type="image/png" data-filename="Image.png" width="8"/></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">)를 구할 수 있다.</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>=&gt;</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [23].png" type="image/png" data-filename="Image.png" width="229"/></span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">  ,</span> <span style="line-height: 160%;"><img src="요약 정리_files/Image [24].png" type="image/png" data-filename="Image.png" width="90"/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-family: 함초롬바탕; font-weight: bold;"><span>    </span>Policy gradient의 단점</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span>- global optimal이 아닌 local optimal에 수렴하기 쉽다</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span>- 분산이 높다</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;"><span>    </span>----------------미완성------------------------</span></div></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm; font-size: 10pt;"><br/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-size: 14pt; color: rgb(54, 101, 238);"><span>    </span>6. Actor Critic</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span>    </span>Policy를 actor로, Q function을 critic(비평가)의 역할로 사용한다.</div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span>    </span>Actor Critic 기초 순서 : <span style="font-weight: bold;">Q Actor &lt; Advantage &lt; MC &lt; TD</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"> </div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span>    </span>1.  <span style="font-weight: bold;">Q Actor-Critic</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span>G의 기댓값이 결국은 Q와 같다는 점을 이용해 G를 아래 식처럼 이용한다.</span><span style="text-indent: -5mm; color: rgb(227, 0, 0);"> </span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span> </span><img src="요약 정리_files/Image [25].png" type="image/png" data-filename="Image.png"/> </div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;">   <span>    </span>                <img src="요약 정리_files/Image [26].png" type="image/png" data-filename="Image.png" width="229"/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-weight: bold;"><span>    </span>장점 :</span> Q 함수를 뉴럴넷으로 근사 가능</div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-weight: bold;"><span>    </span>단점 :</span> 분산(variance)이 높다</div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span>2. </span><span style="text-indent: -5mm; font-weight: bold;">Advantage Actor-Critic</span><span style="text-indent: -5mm;"> </span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span> Q Actor-Critic의 식에서 </span><span style="text-indent: -5mm;">수학적 과정을 진행하여(생략 ..) 아래 식과 같이 변형할 수 있다.</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span> </span><span style="text-indent: -5mm; font-size: 10pt; font-family: 함초롬바탕; line-height: 160%;">  </span><span style="font-family: 함초롬바탕;"><img src="요약 정리_files/Image [27].png" type="image/png" data-filename="Image.png"/></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"> <span>    </span>                    <img src="요약 정리_files/Image [28].png" type="image/png" data-filename="Image.png"/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;">   <span>    </span>                  <img src="요약 정리_files/Image [29].png" type="image/png" data-filename="Image.png"/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-weight: bold;"><span>    </span>장점</span> : 분산(variance)이 낮다 </div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="font-weight: bold;"><span>    </span>단점</span> : 파라미터(theta)가 3개나 되서 연산량이 커짐.</div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span>    </span>3. <span style="font-weight: bold;">MC Actor-Cri</span><span style="text-indent: -5mm; font-weight: bold;">tic</span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span>Monte carlo decision process를 이용한 방법이나, TD 방법에 비해 성능이 떨어져 거의 안 쓰인다. </span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"> </span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span style="text-indent: -5mm;"><span>    </span>4. <span style="font-weight: bold;">TD Actor-Critic</span></span></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span>    </span>Temporal Difference를 이용한 방법 (수학적 원리 생략)</div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><br/></div><div style="margin: 0mm; text-indent: -5mm; text-align: justify;"><span><span>    </span>    </span><img src="요약 정리_files/Image [30].png" type="image/png" data-filename="Image.png"/></div><div><br/></div></span>
</div></body></html> 